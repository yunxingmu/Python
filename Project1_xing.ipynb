{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Yun Xing\n",
   
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section focus on defining various functions that are to be used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython import display\n",
    "#import matplotlib_inline\n",
    "#matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "#load the house sales data\n",
    "\n",
    "data=pd.read_feather('house_sales.ftr')\n",
    "data1=data.copy()\n",
    "data2=data.copy()\n",
    "data3=data.copy()\n",
    "data4=data.copy()\n",
    "rmsles=[]\n",
    "start_date='2021-1-1'\n",
    "\n",
    "#define the evualation metric\n",
    "def rmsle(y_hat, y):\n",
    "    return sum((y-y_hat)**2/len(y))**0.5\n",
    "\n",
    "#define the model evaluation function\n",
    "def model(df,date):   \n",
    "  # training the model\n",
    "    test_start,test_end=pd.Timestamp(2021,2,15),pd.Timestamp(2021,3,1)\n",
    "    train_start=pd.Timestamp(date)\n",
    "    df['Sold On']=pd.to_datetime(df['Sold On'], errors='coerce')\n",
    "    train=df[(df['Sold On']>=train_start)&(df['Sold On']<test_start)]\n",
    "    train=train.drop(['Sold On'],axis=1)\n",
    "    test=df[(df['Sold On']>=test_start)&(df['Sold On']<test_end)]\n",
    "    test=test.drop(['Sold On'],axis=1)\n",
    "    train.shape, test.shape\n",
    "   #evaluate the model \n",
    "    from autogluon.tabular import TabularPredictor\n",
    "    label='Sold Price'\n",
    "    predictor=TabularPredictor(label=label).fit(train,hyperparameters={'GBM':{}})\n",
    "    print(predictor.feature_importance(test))\n",
    "    preds=predictor.predict(test.drop(columns=[label]))\n",
    "    print(rmsle(preds,test[label]))\n",
    "    rmsles.append(rmsle(preds,test[label]))\n",
    "\n",
    "# define a data conversion function that convert currency into numerical values, and areas in to sqrt \n",
    "def conversion(d):\n",
    "    d[\"Id\"] = d[\"Id\"].astype(int)\n",
    "    currency = ['Sold Price', 'Listed Price', 'Tax assessed value', 'Annual tax amount']\n",
    "    for c in currency:\n",
    "        d[c] = d[c].replace(\n",
    "            r'[$,-]', '', regex=True).replace(\n",
    "            r'^\\s*$', np.nan, regex=True).astype(float)\n",
    "    areas = ['Total interior livable area', 'Lot size']\n",
    "    for c in areas:\n",
    "        acres = d[c].str.contains('Acres') == True\n",
    "        col = d[c].replace(r'\\b sqft\\b|\\b Acres\\b|\\b,\\b','', regex=True).astype(float)\n",
    "        col[acres] *= 43560\n",
    "        d[c] = col\n",
    "\n",
    "    floatAttrs = ['Bathrooms', 'Elementary School Score', 'Elementary School Distance', \n",
    "                 'High School Score', 'High School Distance']\n",
    "    for c in floatAttrs:\n",
    "        d[c] = d[c].astype(float)\n",
    "    return d\n",
    "#define a function that convert numerical values to log values \n",
    "def log(p):\n",
    "    c='Sold Price'\n",
    "    if c in p.select_dtypes('float64').columns:\n",
    "        p.loc[:,c]=np.log10(p[c]+1)\n",
    "    return p\n",
    "#define a function for removing data with too many missing values (>30%)\n",
    "def remove(m):\n",
    "    null_sum = m.isnull().sum()\n",
    "    m.columns[null_sum < len(m) * 0.3]  \n",
    "    m.drop(columns=m.columns[null_sum > len(m) * 0.3], inplace=True)\n",
    "    return m\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1. In this task, I will add more features to the original feature list (Bedrooms, Bathrooms, Type, Year Built) and compare the rmsle values. The new features I am adding include: Listed Price, Zip and High School Score. These features will be added to the model in a step-by-step fashion. This task is performed without data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053622/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053622/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    24872\n",
      "Train Data Columns: 4\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.546542675816042, 4.000043427276863, 5.75084, 0.39719)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4651.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.95 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 36\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 36 to 8 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', [])                     : 2 | ['Bathrooms', 'Type']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  2 | ['Bathrooms', 'Type']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('int', ['binned', 'text_special']) : 10 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             :  9 | ['__nlp__.bedroom', '__nlp__.bedroom on', '__nlp__.bedroom on ground', '__nlp__.floor', '__nlp__.ground', ...]\n",
      "\t1.2s = Fit runtime\n",
      "\t4 features in original data used to generate 23 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.97 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 22384, Val Rows: 2488\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.2893\t = Validation score   (root_mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.2893\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.82s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053622/\")\n",
      "Computing feature importance via permutation shuffling for 4 features using 1000 rows with 3 shuffle sets...\n",
      "\t1.15s\t= Expected runtime (0.38s per shuffle set)\n",
      "\t0.65s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            importance    stddev   p_value  n  p99_high   p99_low\n",
      "Bathrooms     0.097697  0.009450  0.001552  3  0.151846  0.043547\n",
      "Type          0.078753  0.007417  0.001472  3  0.121252  0.036254\n",
      "Year built    0.063981  0.012160  0.005914  3  0.133660 -0.005698\n",
      "Bedrooms      0.017627  0.003074  0.004994  3  0.035244  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053626/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053626/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    24872\n",
      "Train Data Columns: 5\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.546542675816042, 4.000043427276863, 5.75084, 0.39719)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2638212574661595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tAvailable Memory:                    3768.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.43 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 36\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 36 to 7 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', [])                     : 3 | ['Bathrooms', 'Type', 'Listed Price']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  3 | ['Bathrooms', 'Type', 'Listed Price']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('int', ['binned', 'text_special']) : 10 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             :  8 | ['__nlp__.bedroom', '__nlp__.bedroom on', '__nlp__.bedroom on ground', '__nlp__.floor', '__nlp__.ground', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t5 features in original data used to generate 23 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.97 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.98s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 22384, Val Rows: 2488\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 0.207466\tvalid_set's rmse: 0.23148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2311\t = Validation score   (root_mean_squared_error)\n",
      "\t2.77s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.2311\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.2s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053626/\")\n",
      "Computing feature importance via permutation shuffling for 5 features using 1000 rows with 3 shuffle sets...\n",
      "\t2.0s\t= Expected runtime (0.67s per shuffle set)\n",
      "\t1.38s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              importance    stddev   p_value  n  p99_high   p99_low\n",
      "Listed Price    0.158504  0.017805  0.002090  3  0.260528  0.056481\n",
      "Type            0.083936  0.009860  0.002284  3  0.140435  0.027436\n",
      "Bathrooms       0.083712  0.013752  0.004438  3  0.162511  0.004912\n",
      "Year built      0.045246  0.017133  0.022311  3  0.143422 -0.052930\n",
      "Bedrooms        0.020705  0.007173  0.018876  3  0.061805 -0.020395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053633/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053633/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    24872\n",
      "Train Data Columns: 6\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.546542675816042, 4.000043427276863, 5.75084, 0.39719)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19656219153257584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tAvailable Memory:                    3644.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.97 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 36\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 36 to 7 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', [])                     : 4 | ['Bathrooms', 'Type', 'Listed Price', 'Zip']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  4 | ['Bathrooms', 'Type', 'Listed Price', 'Zip']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('int', ['binned', 'text_special']) : 10 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             :  8 | ['__nlp__.bedroom', '__nlp__.bedroom on', '__nlp__.bedroom on ground', '__nlp__.floor', '__nlp__.ground', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t6 features in original data used to generate 24 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.02 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.97s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 22384, Val Rows: 2488\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.1996\t = Validation score   (root_mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.1996\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.93s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053633/\")\n",
      "Computing feature importance via permutation shuffling for 6 features using 1000 rows with 3 shuffle sets...\n",
      "\t1.45s\t= Expected runtime (0.48s per shuffle set)\n",
      "\t0.88s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              importance    stddev   p_value  n  p99_high   p99_low\n",
      "Zip             0.097587  0.011129  0.002154  3  0.161359  0.033815\n",
      "Listed Price    0.076654  0.008098  0.001850  3  0.123054  0.030253\n",
      "Type            0.076587  0.007680  0.001667  3  0.120593  0.032582\n",
      "Bathrooms       0.042658  0.008343  0.006256  3  0.090463 -0.005148\n",
      "Bedrooms        0.016980  0.006353  0.021816  3  0.053383 -0.019424\n",
      "Year built      0.013725  0.004152  0.014584  3  0.037514 -0.010063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053637/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053637/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    24872\n",
      "Train Data Columns: 7\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.546542675816042, 4.000043427276863, 5.75084, 0.39719)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18009591625958565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tAvailable Memory:                    3640.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.37 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 36\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 36 to 7 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', [])                     : 5 | ['Bathrooms', 'Type', 'Listed Price', 'Zip', 'High School Score']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  5 | ['Bathrooms', 'Type', 'Listed Price', 'Zip', 'High School Score']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('int', ['binned', 'text_special']) : 10 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             :  8 | ['__nlp__.bedroom', '__nlp__.bedroom on', '__nlp__.bedroom on ground', '__nlp__.floor', '__nlp__.ground', ...]\n",
      "\t1.0s = Fit runtime\n",
      "\t7 features in original data used to generate 25 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.05 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.01s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 22384, Val Rows: 2488\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.1961\t = Validation score   (root_mean_squared_error)\n",
      "\t2.63s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.1961\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3.93s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053637/\")\n",
      "Computing feature importance via permutation shuffling for 7 features using 1000 rows with 3 shuffle sets...\n",
      "\t2.04s\t= Expected runtime (0.68s per shuffle set)\n",
      "\t1.2s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   importance    stddev   p_value  n  p99_high   p99_low\n",
      "Zip                  0.092721  0.007762  0.001164  3  0.137197  0.048246\n",
      "Type                 0.077524  0.006478  0.001160  3  0.114643  0.040406\n",
      "Listed Price         0.075738  0.008283  0.001982  3  0.123203  0.028273\n",
      "Bathrooms            0.041644  0.007923  0.005925  3  0.087042 -0.003753\n",
      "Bedrooms             0.016964  0.006462  0.022561  3  0.053994 -0.020065\n",
      "Year built           0.016328  0.003848  0.009010  3  0.038380 -0.005724\n",
      "High School Score    0.009886  0.000483  0.000397  3  0.012652  0.007121\n",
      "0.17837551642747815\n",
      "\n",
      "\n",
      "Task 1 results: rmsles changes as more features are added, model:GBM\n",
      "\n",
      "The orginal features include:Sold Price, Bedrooms, Bathrooms, Type and Year Built.\n",
      "\n",
      "Original features: 0.2638212574661595\n",
      "\n",
      "Original features plus Listed Price : 0.19656219153257584\n",
      "\n",
      "Original features plus Listed Price and Zip : 0.18009591625958565\n",
      "\n",
      "Original features plus Listed Price and Zip and High School Score : 0.17837551642747815\n"
     ]
    }
   ],
   "source": [
    "# Task 1. Adding more features, no preprosessing on data\n",
    "#convert sold prices to numeric values\n",
    "c='Sold Price'\n",
    "if c in data1.select_dtypes('object').columns:\n",
    "    data1.loc[:,c]=np.log10(pd.to_numeric(data1[c].replace(r'[$,-]','',regex=True))+1)\n",
    "#remove values that are too high (>100 million) or too low (<10,000)\n",
    "data1=data1[(data1['Sold Price']>=4) & (data1['Sold Price']<=8)]\n",
    "features=['Sold Price','Sold On','Bedrooms','Bathrooms','Type','Year built']\n",
    "df1=data1[features].copy()\n",
    "model(df1,start_date)\n",
    "add_feature=['Listed Price','Zip','High School Score']\n",
    "for f in add_feature:\n",
    "    features.append(f)\n",
    "    df1=data1[features].copy()\n",
    "    model(df1,start_date)\n",
    "print('\\n')\n",
    "print('Task 1 results: rmsles changes as more features are added, model:GBM')\n",
    "print('\\nThe orginal features include:Sold Price, Bedrooms, Bathrooms, Type and Year Built.')\n",
    "print('\\nOriginal features:',str(rmsles[0]))\n",
    "print('\\nOriginal features plus',add_feature[0],':',str(rmsles[1]))\n",
    "print('\\nOriginal features plus',add_feature[0],'and',add_feature[1],':',str(rmsles[2]))\n",
    "print('\\nOriginal features plus',add_feature[0],'and',add_feature[1],'and',add_feature[2],':',str(rmsles[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Summary\n",
    "Adding more features leads to decrease in rmsle values. The largest reduction (~0.0673) in rmsle came from adding in Listed Price, adding in Zip further reduced the rmsle by ~0.0165 and adding in high school score resulted in a slight reduction of ~0.00172. \n",
    "\n",
    "\n",
    "Task 2. Modeling with data convertion, in this section, I will use the conversion function to covert the columns that contain currency to numerical values, and columns that contains area (square feet or acres) to square feet and convert that to numerical values. Features used in the modeling include: Bedrooms, Bathrooms, Type, Year Built, Highschool Score, Listed Price and Zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053735/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053735/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    24872\n",
      "Train Data Columns: 7\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.546542675816042, 4.000043427276863, 5.75084, 0.39719)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5274.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.71 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 36\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 36 to 11 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 3 | ['Bathrooms', 'High School Score', 'Listed Price']\n",
      "\t\t('object', [])                     : 2 | ['Type', 'Zip']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  2 | ['Type', 'Zip']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('float', [])                       :  3 | ['Bathrooms', 'High School Score', 'Listed Price']\n",
      "\t\t('int', ['binned', 'text_special']) : 10 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             : 12 | ['__nlp__.bedroom', '__nlp__.bedroom on', '__nlp__.bedroom on ground', '__nlp__.closet', '__nlp__.floor', ...]\n",
      "\t1.0s = Fit runtime\n",
      "\t7 features in original data used to generate 29 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.74 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 22384, Val Rows: 2488\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.1593\t = Validation score   (root_mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.1593\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.72s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053735/\")\n",
      "Computing feature importance via permutation shuffling for 7 features using 1000 rows with 3 shuffle sets...\n",
      "\t2.29s\t= Expected runtime (0.76s per shuffle set)\n",
      "\t0.86s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   importance    stddev   p_value  n  p99_high   p99_low\n",
      "Listed Price         0.367533  0.004371  0.000024  3  0.392580  0.342487\n",
      "Zip                  0.025585  0.012191  0.034024  3  0.095441 -0.044271\n",
      "Type                 0.014005  0.003719  0.011353  3  0.035315 -0.007304\n",
      "Bathrooms            0.009682  0.006493  0.061442  3  0.046888 -0.027524\n",
      "Year built           0.006737  0.002268  0.017879  3  0.019731 -0.006258\n",
      "High School Score    0.003557  0.001140  0.016290  3  0.010090 -0.002976\n",
      "Bedrooms             0.001846  0.001003  0.042939  3  0.007593 -0.003900\n",
      "0.1112193346749113\n",
      "\n",
      "Task 2 results: rmsle values before & after data conversion with features:\n",
      "\n",
      "Before: 0.17837551642747815 After: 0.1112193346749113\n"
     ]
    }
   ],
   "source": [
    "conversion(data2)\n",
    "data2.describe()\n",
    "features2=['Sold Price','Sold On','Bedrooms','Bathrooms','High School Score','Type','Year built','Zip','Listed Price']\n",
    "df2=data2[features2].copy()\n",
    "log(df2)\n",
    "#remove values that are too high (>100 million) or too low (<10,000)\n",
    "df2=df2[(df2['Sold Price']>=4) & (df2['Sold Price']<=8)]\n",
    "model(df2,start_date)\n",
    "features2.remove('Sold On')\n",
    "print('\\nTask 2 results: rmsle values before & after data conversion with features:\\n')\n",
    "print('Before:',str(rmsles[-2]),'After:',str(rmsles[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Summary \n",
    "\n",
    "Adding the Data conversion step resulted in a significant drop (~0.067156) in rmsle value with the same list of features.\n",
    "\n",
    "Task 3: Modeling with data cleansing, i.e. removal of missing values\n",
    "\n",
    "In this task, I added a data cleansing step to remove data with too many missing values (>30%) and compared the rmsle values before and after the cleansing. The list of features remains the same as that in Task 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053754/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053754/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    24872\n",
      "Train Data Columns: 7\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.546542675816042, 4.000043427276863, 5.75084, 0.39719)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4865.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.71 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 36\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 3 | ['Bathrooms', 'High School Score', 'Listed Price']\n",
      "\t\t('object', [])                     : 2 | ['Type', 'Zip']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  2 | ['Type', 'Zip']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('float', [])                       :  3 | ['Bathrooms', 'High School Score', 'Listed Price']\n",
      "\t\t('int', ['binned', 'text_special']) : 10 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             : 37 | ['__nlp__.bedroom', '__nlp__.bedroom master', '__nlp__.bedroom on', '__nlp__.bedroom on ground', '__nlp__.closet', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t7 features in original data used to generate 54 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.99 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.88s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 22384, Val Rows: 2488\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.1594\t = Validation score   (root_mean_squared_error)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.1594\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1.9s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053754/\")\n",
      "Computing feature importance via permutation shuffling for 7 features using 1000 rows with 3 shuffle sets...\n",
      "\t1.51s\t= Expected runtime (0.5s per shuffle set)\n",
      "\t0.82s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   importance    stddev   p_value  n  p99_high   p99_low\n",
      "Listed Price         0.367742  0.004438  0.000024  3  0.393171  0.342313\n",
      "Zip                  0.025397  0.011999  0.033504  3  0.094152 -0.043357\n",
      "Type                 0.013897  0.003834  0.012221  3  0.035865 -0.008071\n",
      "Bathrooms            0.009521  0.006407  0.061797  3  0.046235 -0.027194\n",
      "Year built           0.006619  0.002207  0.017553  3  0.019262 -0.006025\n",
      "High School Score    0.003443  0.001241  0.020333  3  0.010551 -0.003666\n",
      "Bedrooms             0.001843  0.001010  0.043653  3  0.007633 -0.003947\n",
      "0.11093713640043995\n",
      "\n",
      "Task 3 results: removing data with too many missing values\n",
      "Features:,Sold Price,Bedrooms,Bathrooms,High School Score,Type,Year built,Zip,Listed Price\n",
      "Before: 0.1112193346749113 After: 0.11093713640043995\n"
     ]
    }
   ],
   "source": [
    "#Perform data cleansing\n",
    "remove(data3)\n",
    "features3=['Sold Price','Sold On','Bedrooms','Bathrooms','High School Score','Type','Year built','Zip','Listed Price']\n",
    "#performing data conversion\n",
    "conversion(data3)\n",
    "data3.describe()\n",
    "df3=data3[features3].copy()\n",
    "log(df3)\n",
    "#remove values that are too high (>100 million) or too low (<10,000)\n",
    "df3=df3[(df3['Sold Price']>=4) & (df3['Sold Price']<=8)]\n",
    "model(df3,start_date)\n",
    "features3.remove('Sold On')\n",
    "print('\\nTask 3 results: removing data with too many missing values')\n",
    "print('Features:',*features3,sep=',')\n",
    "print('Before:',str(rmsles[-2]),'After:',str(rmsles[-1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Task 3: Summary\n",
    "    Including a data cleasning step led to a slight reduction (~0.000282) in rmsle value with the feature list containing: Bedrooms,Bathrooms, High School Score, Type, Year Built, Zip and Listed Price. \n",
    "\n",
    "Task 4. Adding more training examples\n",
    "\n",
    "    The effect of the training sample size was tested by changing the train_start_date from Jan 1st, 2021 to Aug 1st, 2020, thus increasing the number of training samples significantly. The feature list remains the same as in Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053808/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053808/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    24872\n",
      "Train Data Columns: 7\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.546542675816042, 4.000043427276863, 5.75084, 0.39719)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5025.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.71 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 36\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 3 | ['Bathrooms', 'High School Score', 'Listed Price']\n",
      "\t\t('object', [])                     : 2 | ['Type', 'Zip']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  2 | ['Type', 'Zip']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('float', [])                       :  3 | ['Bathrooms', 'High School Score', 'Listed Price']\n",
      "\t\t('int', ['binned', 'text_special']) : 10 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             : 37 | ['__nlp__.bedroom', '__nlp__.bedroom master', '__nlp__.bedroom on', '__nlp__.bedroom on ground', '__nlp__.closet', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t7 features in original data used to generate 54 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.99 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.95s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 22384, Val Rows: 2488\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.1594\t = Validation score   (root_mean_squared_error)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.1594\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1.97s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053808/\")\n",
      "Computing feature importance via permutation shuffling for 7 features using 1000 rows with 3 shuffle sets...\n",
      "\t1.63s\t= Expected runtime (0.54s per shuffle set)\n",
      "\t0.82s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   importance    stddev   p_value  n  p99_high   p99_low\n",
      "Listed Price         0.367742  0.004438  0.000024  3  0.393171  0.342313\n",
      "Zip                  0.025397  0.011999  0.033504  3  0.094152 -0.043357\n",
      "Type                 0.013897  0.003834  0.012221  3  0.035865 -0.008071\n",
      "Bathrooms            0.009521  0.006407  0.061797  3  0.046235 -0.027194\n",
      "Year built           0.006619  0.002207  0.017553  3  0.019262 -0.006025\n",
      "High School Score    0.003443  0.001241  0.020333  3  0.010551 -0.003666\n",
      "Bedrooms             0.001843  0.001010  0.043653  3  0.007633 -0.003947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053812/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053812/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    63767\n",
      "Train Data Columns: 7\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.954242514264819, 4.0, 5.81586, 0.44186)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11093713640043995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tAvailable Memory:                    5003.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 17.31 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 61\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 61 to 59 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 3 | ['Bathrooms', 'High School Score', 'Listed Price']\n",
      "\t\t('object', [])                     : 2 | ['Type', 'Zip']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  2 | ['Type', 'Zip']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('float', [])                       :  3 | ['Bathrooms', 'High School Score', 'Listed Price']\n",
      "\t\t('int', ['binned', 'text_special']) : 12 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             : 60 | ['__nlp__.bedroom', '__nlp__.bedroom master', '__nlp__.bedroom master bedroom', '__nlp__.bedroom master suite', '__nlp__.bedroom on', ...]\n",
      "\t1.9s = Fit runtime\n",
      "\t7 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 10.78 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.039205231546097515, Train Rows: 61267, Val Rows: 2500\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.2054\t = Validation score   (root_mean_squared_error)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.2054\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3.41s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053812/\")\n",
      "Computing feature importance via permutation shuffling for 7 features using 1000 rows with 3 shuffle sets...\n",
      "\t1.47s\t= Expected runtime (0.49s per shuffle set)\n",
      "\t0.75s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   importance    stddev   p_value  n  p99_high   p99_low\n",
      "Listed Price         0.353504  0.004633  0.000029  3  0.380051  0.326956\n",
      "Zip                  0.041132  0.013184  0.016291  3  0.116678 -0.034414\n",
      "Type                 0.023012  0.001645  0.000849  3  0.032438  0.013587\n",
      "Bathrooms            0.012060  0.003369  0.012516  3  0.031363 -0.007242\n",
      "Year built           0.005741  0.003781  0.059632  3  0.027408 -0.015926\n",
      "High School Score    0.001566  0.000851  0.042925  3  0.006441 -0.003308\n",
      "Bedrooms             0.000406  0.000527  0.156918  3  0.003424 -0.002613\n",
      "0.10579286449971934\n",
      "\n",
      "Task 4 results: adding more examples by changing train_start_date from Jan 1,2021 to Aug 1st, 2020\n",
      "Fewer samples: 0.11093713640043995 More samples: 0.10579286449971934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_date1='2020-8-1'\n",
    "remove(data4)\n",
    "features4=['Sold Price','Sold On','Bedrooms','Bathrooms','High School Score','Type','Year built','Zip','Listed Price']\n",
    "df4=data4[features4].copy()\n",
    "df4.describe()\n",
    "conversion(data4)\n",
    "df4=data4[features4].copy()\n",
    "log(df4)\n",
    "#remove values that are too high (>100 million) or too low (<10,000)\n",
    "df4=df4[(df4['Sold Price']>=4) & (df4['Sold Price']<=8)]\n",
    "model(df4,start_date)\n",
    "model(df4,start_date1)\n",
    "print(\"\\nTask 4 results: adding more examples by changing train_start_date from Jan 1,2021 to Aug 1st, 2020\")\n",
    "print(\"Fewer samples:\",str(rmsles[-2]),\"More samples:\",str(rmsles[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Summary\n",
    "My results showed a further drop(~0.00514) in rmsle value when more trainign samples were added. \n",
    "\n",
    "Task 5. Further improvement of the model by adding new features\n",
    "\n",
    "In this task, I have tested several new features including \"Annual tax amount\",\"Heating\",\"Garage spaces\" and \"Tax assessed value\". The following code only contains that on \"Annual tax\" and \"Heating\" as these two features appeared to be most effective in reducing rmsle values. The train_start_date was set to be Aug 1st, 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053817/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053817/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    63767\n",
      "Train Data Columns: 7\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.954242514264819, 4.0, 5.81586, 0.44186)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4822.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 17.31 MB (0.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 61\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 61 to 49 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 3 | ['Bathrooms', 'Annual tax amount', 'Listed Price']\n",
      "\t\t('object', [])                     : 2 | ['Type', 'Zip']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 1 | ['Bedrooms']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  2 | ['Type', 'Zip']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['Bedrooms']\n",
      "\t\t('float', [])                       :  3 | ['Bathrooms', 'Annual tax amount', 'Listed Price']\n",
      "\t\t('int', ['binned', 'text_special']) : 12 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :  1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             : 50 | ['__nlp__.bedroom', '__nlp__.bedroom master', '__nlp__.bedroom master bedroom', '__nlp__.bedroom master suite', '__nlp__.bedroom on', ...]\n",
      "\t1.9s = Fit runtime\n",
      "\t7 features in original data used to generate 69 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.5 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.01s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.039205231546097515, Train Rows: 61267, Val Rows: 2500\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.196\t = Validation score   (root_mean_squared_error)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.196\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3.13s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053817/\")\n",
      "Computing feature importance via permutation shuffling for 7 features using 1000 rows with 3 shuffle sets...\n",
      "\t1.38s\t= Expected runtime (0.46s per shuffle set)\n",
      "\t0.7s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   importance    stddev   p_value  n  p99_high   p99_low\n",
      "Listed Price         0.329399  0.001469  0.000003  3  0.337814  0.320984\n",
      "Annual tax amount    0.062166  0.003061  0.000404  3  0.079706  0.044626\n",
      "Zip                  0.025051  0.006955  0.012373  3  0.064905 -0.014803\n",
      "Type                 0.014414  0.002601  0.005341  3  0.029319 -0.000492\n",
      "Bathrooms            0.006396  0.002179  0.018285  3  0.018881 -0.006089\n",
      "Year built           0.002181  0.001926  0.094380  3  0.013215 -0.008852\n",
      "Bedrooms            -0.000165  0.000223  0.835464  3  0.001112 -0.001441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220308_053821/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220308_053821/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    63767\n",
      "Train Data Columns: 8\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10179773835749102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tLabel info (max, min, mean, stddev): (7.954242514264819, 4.0, 5.81586, 0.44186)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4817.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.7 MB (0.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Bedrooms', 'Heating']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 323\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 323 to 238 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 3 | ['Bathrooms', 'Annual tax amount', 'Listed Price']\n",
      "\t\t('object', [])                     : 2 | ['Type', 'Zip']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['Year built']\n",
      "\t\t('object', ['text'])               : 2 | ['Bedrooms', 'Heating']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   2 | ['Type', 'Zip']\n",
      "\t\t('category', ['text_as_category'])  :   2 | ['Bedrooms', 'Heating']\n",
      "\t\t('float', [])                       :   3 | ['Bathrooms', 'Annual tax amount', 'Listed Price']\n",
      "\t\t('int', ['binned', 'text_special']) :  24 | ['Bedrooms.char_count', 'Bedrooms.word_count', 'Bedrooms.capital_ratio', 'Bedrooms.lower_ratio', 'Bedrooms.digit_ratio', ...]\n",
      "\t\t('int', ['datetime_as_int'])        :   1 | ['Year built']\n",
      "\t\t('int', ['text_ngram'])             : 239 | ['__nlp__.air', '__nlp__.air central', '__nlp__.air central forced', '__nlp__.air electric', '__nlp__.air fireplace', ...]\n",
      "\t3.9s = Fit runtime\n",
      "\t8 features in original data used to generate 271 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.5 MB (0.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.039205231546097515, Train Rows: 61267, Val Rows: 2500\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM ...\n",
      "/Users/yunxing/py37/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-0.1906\t = Validation score   (root_mean_squared_error)\n",
      "\t2.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.1906\t = Validation score   (root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.99s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220308_053821/\")\n",
      "Computing feature importance via permutation shuffling for 8 features using 1000 rows with 3 shuffle sets...\n",
      "\t2.45s\t= Expected runtime (0.82s per shuffle set)\n",
      "\t1.53s\t= Actual runtime (Completed 3 of 3 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   importance    stddev   p_value  n  p99_high   p99_low\n",
      "Listed Price         0.322060  0.003541  0.000020  3  0.342351  0.301769\n",
      "Annual tax amount    0.065656  0.002001  0.000155  3  0.077120  0.054192\n",
      "Zip                  0.029556  0.008656  0.013710  3  0.079156 -0.020044\n",
      "Type                 0.014721  0.002062  0.003238  3  0.026536  0.002905\n",
      "Bathrooms            0.005271  0.002916  0.044336  3  0.021981 -0.011439\n",
      "Heating              0.004108  0.001597  0.023443  3  0.013262 -0.005045\n",
      "Year built           0.002711  0.001783  0.059480  3  0.012926 -0.007504\n",
      "Bedrooms             0.000388  0.000226  0.048585  3  0.001686 -0.000909\n",
      "0.09855560319201753\n",
      "Features for the lowest rmsle (<0.1):\n",
      ",Sold Price,Bedrooms,Bathrooms,Annual tax amount,Heating,Type,Year built,Zip,Listed Price\n",
      "Test start date: 2020-8-1\n",
      "Data preprocessing: Yes\n",
      "Model: GBM\n",
      "rmsle value: 0.09855560319201753\n"
     ]
    }
   ],
   "source": [
    "# Extra credit, reduce the rmsle to below 0.1\n",
    "data5=data3.copy()\n",
    "features5a=['Sold Price','Sold On','Bedrooms','Bathrooms','Annual tax amount','Type','Year built','Zip','Listed Price']\n",
    "df5a=data5[features5a].copy()\n",
    "log(df5a)\n",
    "features5b=['Sold Price','Sold On','Bedrooms','Bathrooms','Annual tax amount','Heating','Type','Year built','Zip','Listed Price']\n",
    "df5b=data5[features5b].copy()\n",
    "log(df5b)\n",
    "#remove values that are too high (>100 million) or too low (<10,000)\n",
    "df5a=df5a[(df5a['Sold Price']>=4) & (df5a['Sold Price']<=8)]\n",
    "df5b=df5b[(df5b['Sold Price']>=4) & (df5b['Sold Price']<=8)]\n",
    "model(df5a,start_date1)\n",
    "model(df5b,start_date1)\n",
    "rmsles\n",
    "features5b.remove('Sold On')\n",
    "print('Features for the lowest rmsle (<0.1):\\n',*features5b,sep=',')\n",
    "print('Test start date:', start_date1)\n",
    "print('Data preprocessing: Yes')\n",
    "print('Model: GBM')\n",
    "print('rmsle value:',str(rmsles[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5. Summary\n",
    "It appears that adding Annual tax amount and Heating further reduced the rmsle value by ~ 0.00753, thus the ending rmsle value is now below 0.1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
