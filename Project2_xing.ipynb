{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   
    "Author: Yun Xing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: set up the environment and document preprocessing: the documents were loaded into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,sys\n",
    "#nltk.download('stopwords')\n",
    "df = pd.DataFrame(columns=[\"directory\",\"filename\",\"content\"])\n",
    "for (root,dirs, files) in os.walk('mini_newsgroups', topdown=True):\n",
    "    for name in files:\n",
    "        try:\n",
    "            path=os.path.join(root,name)\n",
    "            #print(os.path.join(root,name))\n",
    "            #Read the file and add it to the dataframe as a new row containing the directory, the file name and the content of the file\n",
    "            data=open(path,'r',encoding='ISO-8859-1')\n",
    "            df.loc[len(df.index)]=[root,name,data]\n",
    "        except:\n",
    "            # anything wrong, print them\n",
    "            print(\"error:\", root, name)\n",
    "            None \n",
    "#df.describe()\n",
    "#df.head()\n",
    "#get the number of document N, this number is calculated as the total number of rows of the dataframe subtract by 1\n",
    "#as the first row is a file automaticaly generated by the mac system and does not belong to the data\n",
    "N=df.shape[0]-1\n",
    "print('Document preprocessing complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.2 Data preprocessing: This included gettign content from each document and convert the content into a bag of terms for each document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(data):\n",
    "    body=[]\n",
    "    lineNumber=0\n",
    "    lines=data.readlines()\n",
    "    error =0 \n",
    "        \n",
    "    # get the subject line and line number of the body text\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Subject:\"):\n",
    "            #print(line)\n",
    "            l=line.lstrip('Subject:')\n",
    "            #print(l)\n",
    "            body.append(l)\n",
    "            #print(body)\n",
    "        if line.startswith(\"Lines:\"):\n",
    "            #print(line)\n",
    "            ln=line.lstrip('Lines:')\n",
    "            try:\n",
    "                lineNumber=int(ln)\n",
    "                break\n",
    "            except:\n",
    "                error=1\n",
    "            #print(lineNumber)\n",
    "    #get the body content\n",
    "    doc_body=\"\"\n",
    "    if(error==0 and lineNumber>0):\n",
    "        for line in lines[-lineNumber:]:\n",
    "            bl=line.strip()\n",
    "            body.append(bl)\n",
    "            #print(body)\n",
    "        doc_body=' '.join(body)\n",
    "        #print(doc_body)\n",
    "    return doc_body, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of terms complete\n"
     ]
    }
   ],
   "source": [
    "def get_terms(string):\n",
    "    #remove punctuation from string\n",
    "    #initializing punctuation\n",
    "    punc='''+-!,''.\\n?;:()[]^\"{}<>||_\\/=*#'''\n",
    "    #removing all punctuation in the document\n",
    "    for ele in string:\n",
    "        if ele in punc:\n",
    "            string=string.replace(ele,'')\n",
    "    #print(string)\n",
    "    tokens=string.split(' ')\n",
    "    #print(tokens)          \n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens=[t.lower() for t in tokens]\n",
    "    #print(tokens)\n",
    "    tokens=[t for t in tokens if t not in stop_words]\n",
    "    #print(tokens)\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    terms = [ps.stem(t) for t in tokens]\n",
    "    #print('\\n',terms)\n",
    "    return terms\n",
    "# collect the terms for each document and combine them all into a global_terms list\n",
    "d_terms=[]\n",
    "global_terms=[]\n",
    "for doc in df['content']:\n",
    "    #print(doc)\n",
    "    doc_content,error=get_content(doc)\n",
    "    doc_terms=get_terms(doc_content)\n",
    "    #put all doc_terms in one column (\"d_terms\")\n",
    "    d_terms.append(doc_terms)\n",
    "    #put all doc_terms in one large list\n",
    "    global_terms.extend(doc_terms)\n",
    "#print(d_terms)\n",
    "print('bag of terms complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.1. Create the global terms dictionary with unique terms, assign each term a term_id (or feature_id), starting from 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global terms complete\n"
     ]
    }
   ],
   "source": [
    "#removal duplicate terms in the global_terms list using set()\n",
    "global_terms=list(set(global_terms))\n",
    "#remove the first term which is an empty space\n",
    "del global_terms[0]\n",
    "#print(global_terms)\n",
    "#put the unqiue terms in a dictionary and assign term_ID to each term\n",
    "length=len(global_terms)\n",
    "g_terms={}\n",
    "for j in range(length):\n",
    "    terms={j:global_terms[j]}\n",
    "    g_terms.update(terms)\n",
    "#print(g_terms)\n",
    "print('global terms complete')\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in g_terms.items():\n",
    "        if val==value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.2. Calculate the DF for each term in the global terms dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF complete!\n"
     ]
    }
   ],
   "source": [
    "#put d_terms in a dictionary\n",
    "d_terms1={}\n",
    "l=len(d_terms)\n",
    "for m in range(l):\n",
    "    value=d_terms[m]\n",
    "    key=m\n",
    "    pair={key:value}\n",
    "    d_terms1.update(pair)\n",
    "\n",
    "#calculate the DF for each term & save the results in a dictionary\n",
    "dict_DF={}\n",
    "for term_ID,term in g_terms.items():\n",
    "    DF=0\n",
    "    for doc in d_terms1.values():\n",
    "        if term in doc:\n",
    "            DF+=1\n",
    "    DF=DF/N\n",
    "    dict={term_ID:DF}\n",
    "    dict_DF.update(dict)\n",
    "#print(dict_DF)\n",
    "print('DF complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.3 Assign class ID to each document: the 20 mini_newsgroups were divided into 6 classes and each document was assigned a class_ID based on the directory it belongs. The class IDs were then saved to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assign class ID complete!\n"
     ]
    }
   ],
   "source": [
    "class_labels={'comp.graphics':0,'comp.os.ms-windows.misc':0,'comp.sys.ibm.pc.hardware':0,\n",
    "          'comp.sys.mac.hardware':0, 'comp.windows.x':0,'rec.autos':1, 'rec.motorcycles':1,\n",
    "            'rec.sport.baseball':1, 'rec.sport.hockey':1,'sci.crypt':2, 'sci.electronics':2, \n",
    "              'sci.med':2, 'sci.space':2,'misc.forsale':3,'talk.politics.misc':4, 'talk.politics.guns':4,\n",
    "              'talk.politics.mideast':4,'talk.religion.misc':5, 'alt.atheism':5,'soc.religion.christian':5}\n",
    "def class_ID(string):\n",
    "    class_ID=class_labels.get(string)\n",
    "    return class_ID  \n",
    "\n",
    "\n",
    "#assign class ID to each document \n",
    "c_ID=[]\n",
    "for i in range(1,2001):\n",
    "    dir=df['directory'][i]\n",
    "    s_dir=dir.split('/')[1]\n",
    "    c_ID.append(class_ID(s_dir))\n",
    "\n",
    "print('assign class ID complete!')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.4 Generate trainign data: This included calculating the TF, IDF and TFIDF for each document and saved them to separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features complete!\n"
     ]
    }
   ],
   "source": [
    "def get_key(val):\n",
    "    for key, value in g_terms.items():\n",
    "        if val==value:\n",
    "            return key\n",
    "\n",
    "def features(document):\n",
    "    dict_TF={}\n",
    "    dict_IDF={}\n",
    "    dict_TFIDF={}\n",
    "    ulist=list(set(document))\n",
    "    ulist.remove('')\n",
    "    for t in ulist:\n",
    "        TF=document.count(t)\n",
    "        key=get_key(t)\n",
    "        DF=dict_DF.get(key)\n",
    "        dict1={key:TF}\n",
    "        IDF=np.log10(1/float(DF))\n",
    "        TFIDF=TF*IDF\n",
    "        dict2={key:IDF}\n",
    "        dict3={key:TFIDF}\n",
    "        dict_TF.update(dict1)\n",
    "        dict_IDF.update(dict2)\n",
    "        dict_TFIDF.update(dict3)\n",
    "    return dict_TF, dict_IDF,dict_TFIDF\n",
    "\n",
    "# calculate the TF,IDF and TFIDF for each document\n",
    "doc_TF=[]\n",
    "doc_IDF=[]\n",
    "doc_TFIDF=[]\n",
    "\n",
    "for j in range(len(d_terms)):\n",
    "    doc1=d_terms[j]\n",
    "    doc11_TF,doc11_IDF,doc11_TFIDF=features(doc1)\n",
    "    doc_TF.append(doc11_TF)\n",
    "    doc_IDF.append(doc11_IDF)\n",
    "    doc_TFIDF.append(doc11_TFIDF)\n",
    "\n",
    "print('features complete!')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.5 Test case design,load data to text files and verify the quality of the training data files: test case design was performed manually, the three features (TF,IDF and TFIDF) along with class_ID were written to 3 separate text files and lastly the quality score of each of the 3 training data files was calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test case design, below are three cases chosen from the newsgroup and manually-obtained feature data:\n",
    "file:75895, talk.politics.mideast,class_ID:4, terms: lebanes (TF:4), isra (TF:6),resist(TF:2),bomb(TF:1)\n",
    "file:70337, misc.forsale, class_ID: 3,terms: motorcycl (TF:2),inexpens (TF:1),want(TF:1)\n",
    "file:15322, sci.crypt.,class_ID:2, terms:encrypt (TF:1),feal (TF:3),algorithm (TF:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ID for the three documents:\n",
      "4 3 2\n",
      "term_ID for the selected terms: [35514, 16380, 33601, 26284, 17978, 21346, 26923, 18226, 5486, 25337]\n",
      "TF for \"lebanes\",\"isra\",\"resist\",\"bomb\" in file 75895:\n",
      "4 6 2 1\n",
      "TF for \"motorcycl\",\"inexpens\",\"want\" in file 70337:\n",
      "2 1 1\n",
      "TF for \"encrypt\",\"feal\",\"algorithm\" in file 15322:\n",
      "1 3 2\n",
      "Test cases results match results obtained manually!\n"
     ]
    }
   ],
   "source": [
    "#check if the TF & class_ID results match that obtained manually\n",
    "#The indices for file:78895, 70337 and 15322 are 1,1841 and 714 on the doccument list\n",
    "print('class ID for the three documents:' )\n",
    "print(c_ID[1],c_ID[1841],c_ID[714])\n",
    "#place all chosen terms in a list\n",
    "test_terms=['lebanes','isra','resist','bomb','motorcycl','inexpens','want','encrypt','feal','algorithm']\n",
    "term_ID=[]\n",
    "for term in test_terms:\n",
    "    ID=global_terms.index(term)\n",
    "    term_ID.append(ID)\n",
    "print('term_ID for the selected terms:',term_ID)\n",
    "print('TF for \"lebanes\",\"isra\",\"resist\",\"bomb\" in file 75895:')\n",
    "print(doc_TF[1].get(35514),doc_TF[1].get(16380),doc_TF[1].get(33601),doc_TF[1].get(26284))\n",
    "print('TF for \"motorcycl\",\"inexpens\",\"want\" in file 70337:')\n",
    "print(doc_TF[1841].get(17978),doc_TF[1841].get(21346),doc_TF[1841].get(26923))\n",
    "print('TF for \"encrypt\",\"feal\",\"algorithm\" in file 15322:')\n",
    "print(doc_TF[714].get(18226),doc_TF[714].get(5486),doc_TF[714].get(25337))\n",
    "\n",
    "print('Test cases results match results obtained manually!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write to files complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# opening the TF txt file in 'w' mode\n",
    "file = open('TF.text', 'w+')\n",
    "  \n",
    "for i in range(1,len(c_ID)):\n",
    "    s =\"\"\n",
    "    s += str(c_ID[i]) +\" \"\n",
    "    fids = list(doc_TF[i].keys())\n",
    "    fids.sort()\n",
    "    for fid in fids:\n",
    "        s += str(fid)+\":\"+str(doc_TF[i][fid])+\" \"\n",
    "    file.write(s+'\\n')\n",
    "file.close()\n",
    "\n",
    "# opening the IDF txt file in 'w' mode\n",
    "file = open('IDF.text', 'w+')\n",
    "  \n",
    "for i in range(1,len(c_ID)):\n",
    "    s =\"\"\n",
    "    s += str(c_ID[i]) +\" \"\n",
    "    fids = list(doc_IDF[i].keys())\n",
    "    fids.sort()\n",
    "    for fid in fids:\n",
    "        s += str(fid)+\":\"+str(doc_IDF[i][fid])+\" \"\n",
    "    file.write(s+'\\n')\n",
    "file.close()\n",
    "\n",
    "# opening the IDF txt file in 'w' mode\n",
    "file = open('TFIDF.text', 'w+')\n",
    "  \n",
    "for i in range(1,len(c_ID)):\n",
    "    s =\"\"\n",
    "    s += str(c_ID[i]) +\" \"\n",
    "    fids = list(doc_TFIDF[i].keys())\n",
    "    fids.sort()\n",
    "    for fid in fids:\n",
    "        s += str(fid)+\":\"+str(doc_TFIDF[i][fid])+\" \"\n",
    "    file.write(s+'\\n')\n",
    "file.close()\n",
    "print('write to files complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF score:\n",
      "f1_macro: 0.70 (+/- 0.04)\n",
      "\n",
      "IDF score:\n",
      "f1_macro: 0.74 (+/- 0.04)\n",
      "\n",
      "TFIDF score:\n",
      "f1_macro: 0.75 (+/- 0.04)\n",
      "\n",
      "Project completed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import math\n",
    "\n",
    "def evaluate(training_filename):\n",
    "    feature_vectors, targets = load_svmlight_file(training_filename)\n",
    "    clf = MultinomialNB()\n",
    "    scores = cross_val_score(clf, feature_vectors, targets, cv=5, scoring='f1_macro')\n",
    "    print(\"f1_macro: %0.2f (+/- %0.2f)\" % (scores.mean(), 1.96*scores.std()/math.sqrt(5) ))\n",
    "\n",
    "print('TF score:')\n",
    "evaluate('TF.text')\n",
    "print('\\nIDF score:')\n",
    "evaluate('IDF.text')\n",
    "print('\\nTFIDF score:')\n",
    "evaluate('TFIDF.text')\n",
    "print('\\nProject completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
